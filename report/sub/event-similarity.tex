\documentclass[../report.tex]{subfiles}
\begin{document}

\subsection{Bag of Words} \label{sec:bow}
	Many text similarity models rely on having a collection of words to be treated as features.  They often delimit text by white-space, removing punctuation and normalising to a single case.  The output strings from the SAX algorithm have none of these features and are effectively a single long word so this approach of splitting isn't appropriate
	
	One approach to this is to take a sliding window of length \textit{w} to build to vocabulary \citep{sax-hot}.  This was implemented in Python as follows:

\lstinputlisting[language=Python]{../seismic/similarity/bag_of_words.py}

	For example, given an input string \textit{s} of \verb|abcdefghijkl| and a word length \textit{w} = 5, the function returns the list: 
\begin{verbatim}
	['abdce', 'bdcef', 'dcefg', 'cefgh', 'efghi', 'fghij', 'ghijk', 
	'hijkl']
\end{verbatim}

	Test cases for this function are included in \cref{sec:bow-test}.


\subsection{Jaccard Similarity} \label{sec:jaccard}
\subsubsection{Implementation}
	The Jaccard Similarity Coefficient was implemented in Python as follows (with the case two sets with a cardinality of 0 returning 1).

\lstinputlisting[language=Python]{../seismic/similarity/jaccard.py}

	Test cases are included in \cref{sec:jaccard-test}.  The inputs to the function are the bag-of-words produced from two observations.
	
\subsubsection{Results}

	The Jaccard similarity was run against 138 events with known similarities from the Nabro 2011 data for 4 stations (NAB1..4).  For each event at a given station, a SAX string was produced with the length of the alphabet and the PAA interval as varying parameters between runs.  This was then run through the \textit{bag\_of\_words} function with varying word lengths.  In differing runs, the PAA Interval was set to 5, 10, 25 and 50ms, the alphabet was varied from a length of 4 to 9 characters and the length of the words were set to 5 to 10.
	
	For each run, a similarity matrix was produced comparing every pair of events and the similarities ranked.  This was then aggregated in to a single data frame and the ranks and average ranks compared to whether this was a known similar event.  The results for the following parameters are shown below (these parameters showed the best case for matching known events):
	
\begin{verbatim}
paa_int = 10
alphabet = "abcde"
word_len = 5
\end{verbatim}

\resizebox{\linewidth}{!}{
\pgfplotstabletypeset[%
	col sep=comma,
	string type,
	every head row/.style={
		before row={
			\toprule & \multicolumn{5}{c}{Jaccard Index} & \multicolumn{5}{c}{Rank in Cohort}\\
		},
		after row=\midrule,
	},
	every last row/.style={after row=\bottomrule},
	columns/MeanScore/.style={column name={$\mu$}},
	columns/NAB1-Rank/.style={column name={NAB1}},
	columns/NAB2-Rank/.style={column name={NAB2}},
	columns/NAB3-Rank/.style={column name={NAB3}},
	columns/NAB4-Rank/.style={column name={NAB4}},
	columns/MeanRank/.style={column name={$\mu$}},
]{data/jaccard.csv}}\\

	The table shows the 20 most similar pairs of events sorted by their average rank across the four stations.  The \textit{Match} column shows whether or not the event was a known match.  As can be seen, while some of the matched events score quite highly, there are also many highly scored false positives.  The rankings are of a maximum of 137.  The mean rank for a known match was $48.8$ ($\sigma = 24.7$) and a known non-match was $69.3$ ($\sigma = 23.7$) of 137.  Approximately 69 would be the expected average rank for a non match as it is close to the median rank.  While matched events were more likely to be considered of higher similarity using this method, the effect is only marginal and therefore not a suitable predictor by itself.

\subsection{Vector Space Model (TF-IDF)}
\subsubsection{Implementation}

	To implement the functionality of TF-IDF and the Cosine similarity functions in a high level language such as Python and in a performant way would have taken a significant amount of work.  It has also already implemented and well tested in many open source libraries and for performance reasons is normally constructed in a lower level language such as C.  It was decided for this project to use an opens source library to use a library called Gensim \citep{rehurek_lrec} that has a fairly sensible API and decent documentation.
	
	The following code snippet shows this library being used with the same similarity matrix as the one produced to evaluate the \textit{Jaccard Index} in \cref{sec:jaccard}.  The full code can be found in a Python Notebook under \verb|/notebooks/TFIDF.ipynb| where an aggregate matrix of all of the stations is produced to calculate the combined ranking.  
	
\begin{lstlisting}[language=Python]
# Get a list of events
keys = sorted(sax[st].keys())
# Create a Dict of Matrices per station
tm = {s: SimilarityMatrix(siml.keys()) for s in stations}
	
for st in stations:
	raw_corpus = [sax[st][doc] for doc in keys]
	dictionary = corpora.Dictionary(raw_corpus)
	corpus = [dictionary.doc2bow(t) for t in raw_corpus]
	lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)
	
	for i in range(len(keys)):
		qry = dictionary.doc2bow(raw_corpus[i])
		vec_qry = lsi[qry]
		index = similarities.MatrixSimilarity(lsi[corpus])
		sims = index[vec_qry]
		for j in range(len(sims)):
			tm[st].put(keys[i], keys[j], sims[j])
\end{lstlisting}
	
	As with the \textit{Jaccard Index}, a full range of PAA intervals, alphabet and word lengths were tested.

	
\subsubsection{Results}

	The following shows a table produced in the same way as that for the \textit{Jaccard Index}.  The best scoring parameters were found to be:
\begin{verbatim}
paa_int = 10
alphabet = "abcde"
word_len = 7
\end{verbatim}
	
\resizebox{\linewidth}{!}{
	\pgfplotstabletypeset[%
	col sep=comma,
	string type,
	every head row/.style={
		before row={
			\toprule & \multicolumn{5}{c}{Cosine Similarity} & \multicolumn{5}{c}{Rank in Cohort}\\
		},
		after row=\midrule,
	},
	every last row/.style={after row=\bottomrule},
	columns/MeanScore/.style={column name={$\mu$}},
	columns/NAB1-Rank/.style={column name={NAB1}},
	columns/NAB2-Rank/.style={column name={NAB2}},
	columns/NAB3-Rank/.style={column name={NAB3}},
	columns/NAB4-Rank/.style={column name={NAB4}},
	columns/MeanRank/.style={column name={$\mu$}},
]{data/tfidf.csv}}\\

	Interestingly the performance of the Vector Space Model was even worse than the Jaccard Index in terms of accuracy.  Computationally it also took approximately 6 times more CPU time.  The mean ranking for a match was 57.0 ($\sigma = 22.0$) and a non match was 69.1 ($\sigma = 19.4$).  Again, on average, the rank of a known match was higher but it was again marginal so wouldn't be suitable as a predictor by itself.
	
\end{document}